{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create venv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python3 -m venv venv\n",
    "# !python -m venv venv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PIP Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pypdf\n",
    "!pip install langchain==0.0.312\n",
    "!pip install numpy\n",
    "!pip install -U sentence-transformers\n",
    "!pip install -U scikit-learn\n",
    "!pip install -U pip setuptools wheel\n",
    "!pip install -U spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "!pip install -U matplotlib\n",
    "# !pip install vllm\n",
    "# !pip install huggingface_hub\n",
    "# !pip install --upgrade huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Current Cuda version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"2020-Democratic-Citizenship_Our-Common-Purpose.pdf\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total length of all page contents\n",
    "total_length = sum(len(page.page_content) for page in pages)\n",
    "total_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specific chunking for optimized embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all page contents into single string\n",
    "merged_pages = \"\".join(page.page_content for page in pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_pages[1109:190090])"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using SpaCy to make list of sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
=======
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
>>>>>>> feae939ecec1315e609c03f9dd702fa106e93a5d
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(merged_pages)\n",
    "sentences = list(sent.text for sent in doc.sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentences[1:10])\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KMeans sentence clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Load the Sentence Transformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Generate embeddings for the sentences\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "# Choose an appropriate number of clusters (here we choose 5 as an example)\n",
    "num_clusters = 5\n",
    "\n",
    "# Perform K-means clustering\n",
    "kmeans = KMeans(n_clusters=num_clusters)\n",
    "clusters = kmeans.fit_predict(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ks = range(1, 40)\n",
    "inertias = []\n",
    "for k in ks:\n",
    "    # Create a KMeans instance with k clusters: model\n",
    "    model = KMeans(n_clusters=k)\n",
    "    \n",
    "    # Fit model to samples\n",
    "    model.fit(embeddings)\n",
    "    \n",
    "    # Append the inertia to the list of inertias\n",
    "    inertias.append(model.inertia_)\n",
    "    \n",
    "# Plot ks vs inertias\n",
    "plt.plot(ks, inertias, '-o')\n",
    "plt.xlabel('number of clusters, k')\n",
    "plt.ylabel('inertia')\n",
    "plt.xticks(ks)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering adjacent sentences\n",
    "#### Still splits text segments that contextual belong in the same chunk\n",
    "#### Need to either use layout context or manual chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "# Load the Spacy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def process(text):\n",
    "    doc = nlp(text)\n",
    "    sents = list(doc.sents)\n",
    "    vecs = np.stack([sent.vector / sent.vector_norm for sent in sents])\n",
    "\n",
    "    return sents, vecs\n",
    "\n",
    "def cluster_text(sents, vecs, threshold):\n",
    "    clusters = [[0]]\n",
    "    for i in range(1, len(sents)):\n",
    "        if np.dot(vecs[i], vecs[i-1]) < threshold:\n",
    "            clusters.append([])\n",
    "        clusters[-1].append(i)\n",
    "    \n",
    "    return clusters\n",
    "\n",
    "def clean_text(text):\n",
    "    # Add your text cleaning process here\n",
    "    return text\n",
    "\n",
    "# Initialize the clusters lengths list and final texts list\n",
    "clusters_lens = []\n",
    "final_texts = []\n",
    "\n",
    "# Process the chunk\n",
    "threshold = 0.3\n",
    "sents, vecs = process(merged_pages)\n",
    "\n",
    "# Cluster the sentences\n",
    "clusters = cluster_text(sents, vecs, threshold)\n",
    "\n",
    "for cluster in clusters:\n",
    "    cluster_txt = clean_text(' '.join([sents[i].text for i in cluster]))\n",
    "    cluster_len = len(cluster_txt)\n",
    "    \n",
    "    # Check if the cluster is too short\n",
    "    if cluster_len < 60:\n",
    "        continue\n",
    "    \n",
    "    # Check if the cluster is too long\n",
    "    elif cluster_len > 3000:\n",
    "        threshold = 0.6\n",
    "        sents_div, vecs_div = process(cluster_txt)\n",
    "        reclusters = cluster_text(sents_div, vecs_div, threshold)\n",
    "        \n",
    "        for subcluster in reclusters:\n",
    "            div_txt = clean_text(' '.join([sents_div[i].text for i in subcluster]))\n",
    "            div_len = len(div_txt)\n",
    "            \n",
    "            if div_len < 60 or div_len > 3000:\n",
    "                continue\n",
    "            \n",
    "            clusters_lens.append(div_len)\n",
    "            final_texts.append(div_txt)\n",
    "            \n",
    "    else:\n",
    "        clusters_lens.append(cluster_len)\n",
    "        final_texts.append(cluster_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_chunks = [text for length, text in zip(clusters_lens, final_texts) if length > 2500]\n",
    "\n",
    "len(long_chunks)\n",
    "\n",
    "print(long_chunks[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting the histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(clusters_lens, bins=50, edgecolor='black')\n",
    "plt.title('Distribution of Chunk Lengths')\n",
    "plt.xlabel('Chunk Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layout Parser (needs detectron2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -U layoutparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import layoutparser as lp\n",
    "import cv2\n",
    "\n",
    "image = cv2.imread(\"data/paper-image.jpg\")\n",
    "image = image[..., ::-1]\n",
    "    # Convert the image from BGR (cv2 default loading style)\n",
    "    # to RGB\n",
    "\n",
    "model = lp.Detectron2LayoutModel('lp://PubLayNet/faster_rcnn_R_50_FPN_3x/config',\n",
    "                                 extra_config=[\"MODEL.ROI_HEADS.SCORE_THRESH_TEST\", 0.8],\n",
    "                                 label_map={0: \"Text\", 1: \"Title\", 2: \"List\", 3:\"Table\", 4:\"Figure\"})\n",
    "    # Load the deep layout model from the layoutparser API\n",
    "    # For all the supported model, please check the Model\n",
    "    # Zoo Page: https://layout-parser.readthedocs.io/en/latest/notes/modelzoo.html\n",
    "\n",
    "layout = model.detect(image)\n",
    "    # Detect the layout of the input image\n",
    "\n",
    "lp.draw_box(image, layout, box_width=3)\n",
    "    # Show the detected layout of the input image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login --token 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -u -m vllm.entrypoints.openai.api_server --host 0.0.0.0 --model mistralai/Mistral-7B-v0.1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vector_embedding_myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
